{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "dc": {
     "key": "3"
    },
    "run_control": {
     "frozen": false
    },
    "tags": [
     "context"
    ]
   },
   "source": [
    "## 1. Intro\n",
    "With a lot of applications, came a lot of rejection, with various reasons; high loan, low income levels, or too many inquiries on an individual's credit report, for example. Analyzing these applications one by one is mundane, prone to making mistakes, and time-consuming. This task can be automated with machine learning and pretty much every commercial bank does so nowadays. In this notebook, we will build an credit card approval predictor using machine learning. We'll use the <a href=\"http://archive.ics.uci.edu/ml/datasets/credit+approval\">Credit Card Approval dataset</a> from the UCI Machine Learning Repository. \n",
    "\n",
    "### Plans\n",
    "\n",
    "First, start by taking a look at the dataset.\n",
    "We will see that the dataset has a mix numerical and non-numerical features, it contains values with different ranges, plus that it contains a number of missing entries.\n",
    "We will have to preprocess the dataset to ensure the machine learning model we choose can make good predictions.\n",
    "After our data is in good shape, we will do exploratory data analysis gain initial insights and build intuititions.\n",
    "Finally, we will build a machine learning model that can predict if an application for a credit card will be accepted.\n",
    "\n",
    "First, loading and viewing the dataset. Since this data is confidential, the contributor of the dataset has anonymized the feature names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "dc": {
     "key": "3"
    },
    "tags": [
     "sample_code"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>b</td>\n",
       "      <td>30.83</td>\n",
       "      <td>0.000</td>\n",
       "      <td>u</td>\n",
       "      <td>g</td>\n",
       "      <td>w</td>\n",
       "      <td>v</td>\n",
       "      <td>1.25</td>\n",
       "      <td>t</td>\n",
       "      <td>t</td>\n",
       "      <td>1</td>\n",
       "      <td>f</td>\n",
       "      <td>g</td>\n",
       "      <td>00202</td>\n",
       "      <td>0</td>\n",
       "      <td>+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a</td>\n",
       "      <td>58.67</td>\n",
       "      <td>4.460</td>\n",
       "      <td>u</td>\n",
       "      <td>g</td>\n",
       "      <td>q</td>\n",
       "      <td>h</td>\n",
       "      <td>3.04</td>\n",
       "      <td>t</td>\n",
       "      <td>t</td>\n",
       "      <td>6</td>\n",
       "      <td>f</td>\n",
       "      <td>g</td>\n",
       "      <td>00043</td>\n",
       "      <td>560</td>\n",
       "      <td>+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>a</td>\n",
       "      <td>24.50</td>\n",
       "      <td>0.500</td>\n",
       "      <td>u</td>\n",
       "      <td>g</td>\n",
       "      <td>q</td>\n",
       "      <td>h</td>\n",
       "      <td>1.50</td>\n",
       "      <td>t</td>\n",
       "      <td>f</td>\n",
       "      <td>0</td>\n",
       "      <td>f</td>\n",
       "      <td>g</td>\n",
       "      <td>00280</td>\n",
       "      <td>824</td>\n",
       "      <td>+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>b</td>\n",
       "      <td>27.83</td>\n",
       "      <td>1.540</td>\n",
       "      <td>u</td>\n",
       "      <td>g</td>\n",
       "      <td>w</td>\n",
       "      <td>v</td>\n",
       "      <td>3.75</td>\n",
       "      <td>t</td>\n",
       "      <td>t</td>\n",
       "      <td>5</td>\n",
       "      <td>t</td>\n",
       "      <td>g</td>\n",
       "      <td>00100</td>\n",
       "      <td>3</td>\n",
       "      <td>+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>b</td>\n",
       "      <td>20.17</td>\n",
       "      <td>5.625</td>\n",
       "      <td>u</td>\n",
       "      <td>g</td>\n",
       "      <td>w</td>\n",
       "      <td>v</td>\n",
       "      <td>1.71</td>\n",
       "      <td>t</td>\n",
       "      <td>f</td>\n",
       "      <td>0</td>\n",
       "      <td>f</td>\n",
       "      <td>s</td>\n",
       "      <td>00120</td>\n",
       "      <td>0</td>\n",
       "      <td>+</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  0      1      2  3  4  5  6     7  8  9   10 11 12     13   14 15\n",
       "0  b  30.83  0.000  u  g  w  v  1.25  t  t   1  f  g  00202    0  +\n",
       "1  a  58.67  4.460  u  g  q  h  3.04  t  t   6  f  g  00043  560  +\n",
       "2  a  24.50  0.500  u  g  q  h  1.50  t  f   0  f  g  00280  824  +\n",
       "3  b  27.83  1.540  u  g  w  v  3.75  t  t   5  t  g  00100    3  +\n",
       "4  b  20.17  5.625  u  g  w  v  1.71  t  f   0  f  s  00120    0  +"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import pandas\n",
    "import pandas as pd\n",
    "\n",
    "# Load dataset\n",
    "cc_apps = pd.read_csv('datasets/cc_approvals.data', header=None)\n",
    "\n",
    "# Inspect data\n",
    "cc_apps.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "dc": {
     "key": "10"
    },
    "run_control": {
     "frozen": false
    },
    "tags": [
     "context"
    ]
   },
   "source": [
    "## 2. Inspecting data\n",
    "The dataset may appear a bit confusing, let's try to figure out what's important features of a credit card application. The features of this dataset have been anonymized to protect the privacy, but <a href=\"http://rstudio-pubs-static.s3.amazonaws.com/73039_9946de135c0a49daa7a0a9eda4a67a72.html\">this blog</a> gives us an overview of the probable features. The probable features in a typical credit card application are <code>Gender</code>, <code>Age</code>, <code>Debt</code>, <code>Married</code>, <code>BankCustomer</code>, <code>EducationLevel</code>, <code>Ethnicity</code>, <code>YearsEmployed</code>, <code>PriorDefault</code>, <code>Employed</code>, <code>CreditScore</code>, <code>DriversLicense</code>, <code>Citizen</code>, <code>ZipCode</code>, <code>Income</code> and finally the <code>ApprovalStatus</code>. This gives us a pretty good starting point, and we can map these features with respect to the columns in the output.\n",
    "\n",
    "As we can see from our data, the dataset has a mixture of numerical and non-numerical features, it needs to be changed into numerical data before we fit it into our machine learning model. This can be fixed with some preprocessing. Before we do that, let's do some EDA to learn about the data and to see if theres any issues to be fixed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "dc": {
     "key": "10"
    },
    "tags": [
     "sample_code"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               2           7          10             14\n",
      "count  690.000000  690.000000  690.00000     690.000000\n",
      "mean     4.758725    2.223406    2.40000    1017.385507\n",
      "std      4.978163    3.346513    4.86294    5210.102598\n",
      "min      0.000000    0.000000    0.00000       0.000000\n",
      "25%      1.000000    0.165000    0.00000       0.000000\n",
      "50%      2.750000    1.000000    0.00000       5.000000\n",
      "75%      7.207500    2.625000    3.00000     395.500000\n",
      "max     28.000000   28.500000   67.00000  100000.000000\n",
      "\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 690 entries, 0 to 689\n",
      "Data columns (total 16 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   0       690 non-null    object \n",
      " 1   1       690 non-null    object \n",
      " 2   2       690 non-null    float64\n",
      " 3   3       690 non-null    object \n",
      " 4   4       690 non-null    object \n",
      " 5   5       690 non-null    object \n",
      " 6   6       690 non-null    object \n",
      " 7   7       690 non-null    float64\n",
      " 8   8       690 non-null    object \n",
      " 9   9       690 non-null    object \n",
      " 10  10      690 non-null    int64  \n",
      " 11  11      690 non-null    object \n",
      " 12  12      690 non-null    object \n",
      " 13  13      690 non-null    object \n",
      " 14  14      690 non-null    int64  \n",
      " 15  15      690 non-null    object \n",
      "dtypes: float64(2), int64(2), object(12)\n",
      "memory usage: 86.4+ KB\n",
      "None\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>685</th>\n",
       "      <td>b</td>\n",
       "      <td>21.08</td>\n",
       "      <td>10.085</td>\n",
       "      <td>y</td>\n",
       "      <td>p</td>\n",
       "      <td>e</td>\n",
       "      <td>h</td>\n",
       "      <td>1.25</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>0</td>\n",
       "      <td>f</td>\n",
       "      <td>g</td>\n",
       "      <td>00260</td>\n",
       "      <td>0</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>686</th>\n",
       "      <td>a</td>\n",
       "      <td>22.67</td>\n",
       "      <td>0.750</td>\n",
       "      <td>u</td>\n",
       "      <td>g</td>\n",
       "      <td>c</td>\n",
       "      <td>v</td>\n",
       "      <td>2.00</td>\n",
       "      <td>f</td>\n",
       "      <td>t</td>\n",
       "      <td>2</td>\n",
       "      <td>t</td>\n",
       "      <td>g</td>\n",
       "      <td>00200</td>\n",
       "      <td>394</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>687</th>\n",
       "      <td>a</td>\n",
       "      <td>25.25</td>\n",
       "      <td>13.500</td>\n",
       "      <td>y</td>\n",
       "      <td>p</td>\n",
       "      <td>ff</td>\n",
       "      <td>ff</td>\n",
       "      <td>2.00</td>\n",
       "      <td>f</td>\n",
       "      <td>t</td>\n",
       "      <td>1</td>\n",
       "      <td>t</td>\n",
       "      <td>g</td>\n",
       "      <td>00200</td>\n",
       "      <td>1</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>688</th>\n",
       "      <td>b</td>\n",
       "      <td>17.92</td>\n",
       "      <td>0.205</td>\n",
       "      <td>u</td>\n",
       "      <td>g</td>\n",
       "      <td>aa</td>\n",
       "      <td>v</td>\n",
       "      <td>0.04</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>0</td>\n",
       "      <td>f</td>\n",
       "      <td>g</td>\n",
       "      <td>00280</td>\n",
       "      <td>750</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>689</th>\n",
       "      <td>b</td>\n",
       "      <td>35.00</td>\n",
       "      <td>3.375</td>\n",
       "      <td>u</td>\n",
       "      <td>g</td>\n",
       "      <td>c</td>\n",
       "      <td>h</td>\n",
       "      <td>8.29</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>0</td>\n",
       "      <td>t</td>\n",
       "      <td>g</td>\n",
       "      <td>00000</td>\n",
       "      <td>0</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    0      1       2  3  4   5   6     7  8  9   10 11 12     13   14 15\n",
       "685  b  21.08  10.085  y  p   e   h  1.25  f  f   0  f  g  00260    0  -\n",
       "686  a  22.67   0.750  u  g   c   v  2.00  f  t   2  t  g  00200  394  -\n",
       "687  a  25.25  13.500  y  p  ff  ff  2.00  f  t   1  t  g  00200    1  -\n",
       "688  b  17.92   0.205  u  g  aa   v  0.04  f  f   0  f  g  00280  750  -\n",
       "689  b  35.00   3.375  u  g   c   h  8.29  f  f   0  t  g  00000    0  -"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print summary statistics\n",
    "cc_apps_description = cc_apps.describe()\n",
    "print(cc_apps_description)\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "# Print DataFrame information\n",
    "cc_apps_info = cc_apps.info()\n",
    "print(cc_apps_info)\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "# Inspect missing values in the dataset\n",
    "cc_apps.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "dc": {
     "key": "17"
    },
    "run_control": {
     "frozen": false
    },
    "tags": [
     "context"
    ]
   },
   "source": [
    "## 3. Handling the missing values (part i)\n",
    "Before any process can be done to the data, we need to see if there are any issues:\n",
    "\n",
    "* Our dataset contains both numeric and non-numeric data (<code>float64</code>, <code>int64</code> and <code>object</code>).  All non-numeric must be encoded.\n",
    "* The dataset also contains values from several ranges. Later we will scale it. \n",
    "* The dataset has missing values, labeled with '?'.\n",
    "\n",
    "Now, temporarily replace missing values question marks with NaN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "dc": {
     "key": "17"
    },
    "tags": [
     "sample_code"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    0      1       2  3  4   5   6     7  8  9   10 11 12     13   14 15\n",
      "685  b  21.08  10.085  y  p   e   h  1.25  f  f   0  f  g  00260    0  -\n",
      "686  a  22.67   0.750  u  g   c   v  2.00  f  t   2  t  g  00200  394  -\n",
      "687  a  25.25  13.500  y  p  ff  ff  2.00  f  t   1  t  g  00200    1  -\n",
      "688  b  17.92   0.205  u  g  aa   v  0.04  f  f   0  f  g  00280  750  -\n",
      "689  b  35.00   3.375  u  g   c   h  8.29  f  f   0  t  g  00000    0  -\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>685</th>\n",
       "      <td>b</td>\n",
       "      <td>21.08</td>\n",
       "      <td>10.085</td>\n",
       "      <td>y</td>\n",
       "      <td>p</td>\n",
       "      <td>e</td>\n",
       "      <td>h</td>\n",
       "      <td>1.25</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>0</td>\n",
       "      <td>f</td>\n",
       "      <td>g</td>\n",
       "      <td>00260</td>\n",
       "      <td>0</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>686</th>\n",
       "      <td>a</td>\n",
       "      <td>22.67</td>\n",
       "      <td>0.750</td>\n",
       "      <td>u</td>\n",
       "      <td>g</td>\n",
       "      <td>c</td>\n",
       "      <td>v</td>\n",
       "      <td>2.00</td>\n",
       "      <td>f</td>\n",
       "      <td>t</td>\n",
       "      <td>2</td>\n",
       "      <td>t</td>\n",
       "      <td>g</td>\n",
       "      <td>00200</td>\n",
       "      <td>394</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>687</th>\n",
       "      <td>a</td>\n",
       "      <td>25.25</td>\n",
       "      <td>13.500</td>\n",
       "      <td>y</td>\n",
       "      <td>p</td>\n",
       "      <td>ff</td>\n",
       "      <td>ff</td>\n",
       "      <td>2.00</td>\n",
       "      <td>f</td>\n",
       "      <td>t</td>\n",
       "      <td>1</td>\n",
       "      <td>t</td>\n",
       "      <td>g</td>\n",
       "      <td>00200</td>\n",
       "      <td>1</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>688</th>\n",
       "      <td>b</td>\n",
       "      <td>17.92</td>\n",
       "      <td>0.205</td>\n",
       "      <td>u</td>\n",
       "      <td>g</td>\n",
       "      <td>aa</td>\n",
       "      <td>v</td>\n",
       "      <td>0.04</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>0</td>\n",
       "      <td>f</td>\n",
       "      <td>g</td>\n",
       "      <td>00280</td>\n",
       "      <td>750</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>689</th>\n",
       "      <td>b</td>\n",
       "      <td>35.00</td>\n",
       "      <td>3.375</td>\n",
       "      <td>u</td>\n",
       "      <td>g</td>\n",
       "      <td>c</td>\n",
       "      <td>h</td>\n",
       "      <td>8.29</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>0</td>\n",
       "      <td>t</td>\n",
       "      <td>g</td>\n",
       "      <td>00000</td>\n",
       "      <td>0</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    0      1       2  3  4   5   6     7  8  9   10 11 12     13   14 15\n",
       "685  b  21.08  10.085  y  p   e   h  1.25  f  f   0  f  g  00260    0  -\n",
       "686  a  22.67   0.750  u  g   c   v  2.00  f  t   2  t  g  00200  394  -\n",
       "687  a  25.25  13.500  y  p  ff  ff  2.00  f  t   1  t  g  00200    1  -\n",
       "688  b  17.92   0.205  u  g  aa   v  0.04  f  f   0  f  g  00280  750  -\n",
       "689  b  35.00   3.375  u  g   c   h  8.29  f  f   0  t  g  00000    0  -"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import numpy\n",
    "import numpy as np\n",
    "\n",
    "# Inspect missing values in the dataset\n",
    "print(cc_apps.tail())\n",
    "\n",
    "# Replace the '?'s with NaN\n",
    "cc_apps = cc_apps.replace('?', np.nan)\n",
    "\n",
    "# Inspect the missing values again\n",
    "cc_apps.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "dc": {
     "key": "24"
    },
    "run_control": {
     "frozen": false
    },
    "tags": [
     "context"
    ]
   },
   "source": [
    "## 4. Handling the missing values (part ii)\n",
    "Replacing missing values from '?' to NaN will help us treating this missing value issue.\n",
    "\n",
    "To treat missing values, theoritically we can just exclude missing values from our dataset, but that means we are losing other valuable information contained in other features. So, unless theres specific situation where data cannot be syntethically generated, removing uncomplete observation should be a last resort decision.\n",
    "\n",
    "To avoid this problem, we are going to impute the missing values with a strategy called mean imputation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "dc": {
     "key": "24"
    },
    "tags": [
     "sample_code"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     12\n",
      "1     12\n",
      "2      0\n",
      "3      6\n",
      "4      6\n",
      "5      9\n",
      "6      9\n",
      "7      0\n",
      "8      0\n",
      "9      0\n",
      "10     0\n",
      "11     0\n",
      "12     0\n",
      "13    13\n",
      "14     0\n",
      "15     0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Impute the missing values with mean imputation\n",
    "cc_apps.fillna(cc_apps.mean(), inplace=True)\n",
    "\n",
    "# Count the number of NaNs in the dataset to verify\n",
    "print(cc_apps.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "dc": {
     "key": "31"
    },
    "run_control": {
     "frozen": false
    },
    "tags": [
     "context"
    ]
   },
   "source": [
    "## 5. Handling the missing values (part iii)\n",
    "We have imputed the missing values in the numeric columns. There are still missing values present in non-numeric values. We couldnt calculate the mean of a letter, right?. well you couldnt, for this issue, we are going to impute the missing values using the most frequent value in that column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "dc": {
     "key": "31"
    },
    "tags": [
     "sample_code"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     0\n",
      "1     0\n",
      "2     0\n",
      "3     0\n",
      "4     0\n",
      "5     0\n",
      "6     0\n",
      "7     0\n",
      "8     0\n",
      "9     0\n",
      "10    0\n",
      "11    0\n",
      "12    0\n",
      "13    0\n",
      "14    0\n",
      "15    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Iterate over each column of cc_apps\n",
    "for col in cc_apps.columns:\n",
    "    # Check if the column is of object type\n",
    "    if cc_apps[col].dtypes == 'object':\n",
    "        # Impute with the most frequent value\n",
    "        cc_apps = cc_apps.fillna(cc_apps[col].value_counts().index[0])\n",
    "\n",
    "# Count the number of NaNs in the dataset and print the counts to verify\n",
    "print(cc_apps.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "dc": {
     "key": "38"
    },
    "run_control": {
     "frozen": false
    },
    "tags": [
     "context"
    ]
   },
   "source": [
    "## 6. Preprocessing data (part i)\n",
    "All missing values is now handled.\n",
    "\n",
    "There's still some essential steps to do, most machine learning model (even machine learning that accept categorical data will train faster with encoding) only accept numeric values, so it is important to convert the non numeric data into a numeric data. Not only that, we have to split the data into two sets; training set and test set. Also, dont forget to scale the data so we have uniform ranges across all of our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "dc": {
     "key": "38"
    },
    "tags": [
     "sample_code"
    ]
   },
   "outputs": [],
   "source": [
    "# Import LabelEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Instantiate LabelEncoder\n",
    "le = LabelEncoder()\n",
    "\n",
    "# Iterate over all the values of each column and extract their dtypes\n",
    "for col in cc_apps.columns:\n",
    "    # Compare if the dtype is object\n",
    "    if cc_apps[col].dtypes=='object':\n",
    "    # Use LabelEncoder to do the numeric transformation\n",
    "        cc_apps[col]=le.fit_transform(cc_apps[col])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "dc": {
     "key": "45"
    },
    "run_control": {
     "frozen": false
    },
    "tags": [
     "context"
    ]
   },
   "source": [
    "## 7. Splitting the dataset into train and test sets\n",
    "\n",
    "Our data is now in good shape. Now, we split our data into train set and test set for two different phases of machine learning modeling: training and testing. The idea is that the model we going to train is not supposedly to 'know' what it will predict after the training. Also applies in scaling process.\n",
    "\n",
    "### Feature selection\n",
    "Features like <code>DriversLicense</code> and <code>ZipCode</code> are not as important as the other features in the dataset for predicting credit card approvals. We should drop them to design our machine learning model with the best set of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "dc": {
     "key": "45"
    },
    "tags": [
     "sample_code"
    ]
   },
   "outputs": [],
   "source": [
    "# Import train_test_split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Drop the features 11 and 13 and convert the DataFrame to a NumPy array\n",
    "cc_apps = cc_apps.drop([11, 13], axis=1)\n",
    "cc_apps = cc_apps.to_numpy()\n",
    "\n",
    "# Segregate features and labels into separate variables\n",
    "X,y = cc_apps[:,0:13] , cc_apps[:,13]\n",
    "\n",
    "# Split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                y,\n",
    "                                test_size=.33,\n",
    "                                random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "dc": {
     "key": "52"
    },
    "run_control": {
     "frozen": false
    },
    "tags": [
     "context"
    ]
   },
   "source": [
    "## 8. Preprocessing data (part ii)\n",
    "Our data is not splitted into two sets. final preprocessing step is scaling.\n",
    "\n",
    "Why do we do this? lets simplify our problem to predict credit card approval from 13 features to 2 features, for example we only using <code>CreditScore</code> and <code>Income</code> to predict Approval, <code>CreditScore</code> usually come with up to 2 (3 if you are wilding) digit numbers, compared to <code>Income</code> that have (in this data) 4 digits up to 5 digits. If we fit the model using the data without scaling it first, the model will heavily biased towards income.\n",
    "\n",
    "Thats why scaling is important, here we are going to transform every features so it will have range between 0 to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "dc": {
     "key": "52"
    },
    "tags": [
     "sample_code"
    ]
   },
   "outputs": [],
   "source": [
    "# Import MinMaxScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Instantiate MinMaxScaler and use it to rescale X_train and X_test\n",
    "scaler = MinMaxScaler(feature_range=(0,1))\n",
    "rescaledX_train = scaler.fit_transform(X_train)\n",
    "rescaledX_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "dc": {
     "key": "59"
    },
    "run_control": {
     "frozen": false
    },
    "tags": [
     "context"
    ]
   },
   "source": [
    "## 9. Model Fitting\n",
    "This is a <a href=\"https://en.wikipedia.org/wiki/Statistical_classification\">classification</a> task, a binary one, we only predict if an individuals is accepted or not. <a href=\"http://archive.ics.uci.edu/ml/machine-learning-databases/credit-screening/crx.names\">According to UCI</a>, our dataset contains more instances that correspond to \"Denied\" status than instances corresponding to \"Approved\" status. Specifically, out of 690 instances, there are 383 (55.5%) applications that got denied and 307 (44.5%) applications that got approved.\n",
    "\n",
    "We will use this information as a benchmark. A good machine learning model should be able to closely predict the status of the applications with respect to these statistics.\n",
    "    \n",
    "Which model should we pick? If we are to assume that the features we used are correlated to the target variable, simply, we can use Logistic Regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "dc": {
     "key": "59"
    },
    "tags": [
     "sample_code"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import LogisticRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Instantiate a LogisticRegression classifier with default parameter values\n",
    "logreg = LogisticRegression()\n",
    "\n",
    "# Fit logreg to the train set\n",
    "logreg.fit(rescaledX_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "dc": {
     "key": "66"
    },
    "run_control": {
     "frozen": false
    },
    "tags": [
     "context"
    ]
   },
   "source": [
    "## 10. Making predictions and evaluations\n",
    "How well does our model perform?\n",
    "\n",
    "Evaluate our model on the test set with respect to classification accuracy. Also take a look the model's confusion matrix. In the case of predicting credit card applications, it is equally important to see if our model is able to predict the approval status of the applications also denied status that originally got denied. If our model is not performing well in this aspect, then it might end up approving the application that should have been approved.\n",
    "\n",
    "Also add classification so we dont have to manually calculate the recall and precision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "dc": {
     "key": "66"
    },
    "tags": [
     "sample_code"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of logistic regression classifier:  0.8421052631578947\n",
      "[[94  9]\n",
      " [27 98]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.78      0.91      0.84       103\n",
      "         1.0       0.92      0.78      0.84       125\n",
      "\n",
      "    accuracy                           0.84       228\n",
      "   macro avg       0.85      0.85      0.84       228\n",
      "weighted avg       0.85      0.84      0.84       228\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import confusion_matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Use logreg to predict instances from the test set and store it\n",
    "y_pred = logreg.predict(rescaledX_test)\n",
    "\n",
    "# Get the accuracy score of logreg model and print it\n",
    "print(\"Accuracy of logistic regression classifier: \", logreg.score(rescaledX_test, y_test))\n",
    "\n",
    "# Print the confusion matrix of the logreg model\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "# Classification Report\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "dc": {
     "key": "73"
    },
    "run_control": {
     "frozen": false
    },
    "tags": [
     "context"
    ]
   },
   "source": [
    "## 11. Tuning Model\n",
    "The model is decent, it yields 84% accuracy.\n",
    "\n",
    "For confusion matrix, the two elements on our first row are True Negatives and False Negatives repectively, and in the second row are our False Positve and True Positive respectively, so the model predict 'Denied' 94 correct and 9 false, on the 'Approved' status, the model predict 27 false, and 98 correct predictions.\n",
    "\n",
    "To improve the model further. We can tune the model using a <a href=\"https://machinelearningmastery.com/how-to-tune-algorithm-parameters-with-scikit-learn/\">grid search</a> of the model parameters to improve the model's ability to predict credit card approvals.\n",
    "\n",
    "<a href=\"http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\">scikit-learn's implementation of logistic regression</a> consists of different hyperparameters but we will grid search over the following two:\n",
    "* tol\n",
    "* max_iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "dc": {
     "key": "73"
    },
    "tags": [
     "sample_code"
    ]
   },
   "outputs": [],
   "source": [
    "# Import GridSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define the grid of values for tol and max_iter\n",
    "tol = [0.01, 0.001, 0.0001]\n",
    "max_iter = [100, 150, 200]\n",
    "\n",
    "# Create a dictionary where tol and max_iter are keys and the lists of their values are corresponding values\n",
    "param_grid = dict(tol=tol, max_iter=max_iter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "dc": {
     "key": "80"
    },
    "run_control": {
     "frozen": false
    },
    "tags": [
     "context"
    ]
   },
   "source": [
    "## 12. Finding the best performing model\n",
    "We have defined the grid of hyperparameter values and put them into a single dictionary format which <code>GridSearchCV()</code> expects as its parameters. Now see if the model is improving.\n",
    "\n",
    "Like any other sklearn classes, instantiate <code>GridSearchCV()</code> with <code>logreg</code> model as its estimator.\n",
    "\n",
    "We will also instruct <code>GridSearchCV()</code> to perform a <a href=\"https://www.dataschool.io/machine-learning-with-scikit-learn/\">cross-validation</a> of five folds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "dc": {
     "key": "80"
    },
    "tags": [
     "sample_code"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of tuned logistic regression classifier:  0.8421052631578947\n",
      "[[94  9]\n",
      " [27 98]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.78      0.91      0.84       103\n",
      "         1.0       0.92      0.78      0.84       125\n",
      "\n",
      "    accuracy                           0.84       228\n",
      "   macro avg       0.85      0.85      0.84       228\n",
      "weighted avg       0.85      0.84      0.84       228\n",
      "\n",
      "Best using {'max_iter': 100, 'tol': 0.01}\n"
     ]
    }
   ],
   "source": [
    "# Instantiate GridSearchCV with the required parameters\n",
    "grid_model = GridSearchCV(estimator=logreg, param_grid=param_grid, cv=5)\n",
    "\n",
    "# Fit grid_model to the data\n",
    "grid_model.fit(rescaledX_train, y_train)\n",
    "\n",
    "#predict using tuned model\n",
    "grid_model_pred = grid_model.predict(rescaledX_test)\n",
    "\n",
    "# Get the accuracy score of tuned logreg model and print it\n",
    "print(\"Accuracy of tuned logistic regression classifier: \", grid_model.score(rescaledX_test, y_test))\n",
    "\n",
    "# Print the confusion matrix of the logreg model\n",
    "print(confusion_matrix(y_test, grid_model_pred))\n",
    "\n",
    "# Classification Report\n",
    "print(classification_report(y_test, grid_model_pred))\n",
    "\n",
    "# Summarize results\n",
    "best_params =grid_model.best_params_\n",
    "print(\"Best using %s\" % (best_params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0001\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "hyperparam = ['tol', 'max_iter']\n",
    "for i in hyperparam:\n",
    "    print(logreg.get_params().get(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After performing grid search the model decided to use 0.0001 as its best 'tol' parameter and the model doesnt seems to improve compared the default logistic regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 13. Closing\n",
    "While building this credit card predictor, we covered some of the most widely-known preprocessing steps such as <strong>scaling</strong>, <strong>encoding</strong>, and <strong>imputation</strong>. We finished with some <strong>machine learning</strong> model to predict if an individual's application would get approved or not given some information about that person."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
